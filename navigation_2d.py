""" 2D navigation environment """
import gym
import matplotlib.pyplot as plt
import numpy as np
from gym import spaces
from gym.utils import seeding

from math import pi, cos, sin, pow, sqrt

HORIZON = 100

START = [0.0, 0.0]


def dist_2_nogo(x, y):
    dist_fst = sqrt(pow(x - 0.25, 2) + pow(y - 0.25, 2))
    dist_snd = sqrt(pow(x + 0.25, 2) + pow(y - 0.25, 2))
    dist_trd = sqrt(pow(x - 0.25, 2) + pow(y + 0.25, 2))
    dist_frt = sqrt(pow(x + 0.25, 2) + pow(y + 0.25, 2))
    min_dist = min([dist_fst, dist_snd, dist_trd, dist_frt])
    return min_dist


def is_nogo(x, y):
    """Check if agent is in the nogo zone"""
    fst_square = (0.2 < x < 0.3) and (0.2 < y < 0.3)
    snd_square = (-0.3 < x < -0.2) and (0.2 < y < 0.3)
    trd_square = (0.2 < x < 0.3) and (-0.3 < y < -0.2)
    frt_square = (-0.3 < x < -0.2) and (-0.3 < y < -0.2)
    if fst_square or snd_square or trd_square or frt_square:
        return True
    return False


class Navigation2DEnv(gym.Env):
    """2D navigation problems, as described in [1]. The code is adapted from 
    https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/maml_examples/point_env_randgoal.py

    At each time step, the 2D agent takes an action (its velocity, clipped in
    [-0.1, 0.1]), and receives a penalty equal to its L2 distance to the goal 
    position (ie. the reward is `-distance`). The 2D navigation tasks are 
    generated by sampling goal positions from the uniform distribution 
    on [-0.5, 0.5]^2.

    [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic 
        Meta-Learning for Fast Adaptation of Deep Networks", 2017 
        (https://arxiv.org/abs/1703.03400)
    """

    def __init__(self, task={}):
        super(Navigation2DEnv, self).__init__()

        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32
        )
        self.action_space = spaces.Box(low=-0.1, high=0.1, shape=(2,), dtype=np.float32)

        self._task = task
        self._goal = task.get("goal", np.zeros(2, dtype=np.float32))
        self._state = np.array(START)  # np.zeros(2, dtype=np.float32)
        self.seed()
        self.horizon = HORIZON
        self.cummulative_reward = 0
        self.episode_x_path = []
        self.episode_y_path = []

        self.task_sequence = [[0.7, 0.35], [-0.7, -0.35], [0.7, -0.35], [-0.7, 0.35]]

    def _sample_ring_task(self):
        radius = self.np_random.uniform(0.3, 0.5, size=(1, 1))[0][0]
        alpha = self.np_random.uniform(0.0, 1.0, size=(1, 1)) * 2 * pi
        alpha = alpha[0][0]
        goal = np.array([[radius * cos(alpha), radius * sin(alpha)]])
        return goal

    def _sample_square_wth_nogo_zone(self):
        rand_x = self.np_random.uniform(-0.5, 0.5, size=(1, 1))[0][0]
        if (0.2 <= rand_x <= 0.3) or (-0.3 <= rand_x <= -0.2):
            # If random x could be in the no-go zone
            # Sample randomly from four slices
            dart = self.np_random.uniform(0.0, 1.0, size=(1, 1))[0][0]
            if dart <= 0.25:
                rand_y = self.np_random.uniform(-0.5, -0.3, size=(1, 1))[0][0]
            elif 0.25 < dart <= 0.75:
                rand_y = self.np_random.uniform(-0.2, 0.2, size=(1, 1))[0][0]
            else:
                rand_y = self.np_random.uniform(0.3, 0.5, size=(1, 1))[0][0]
        else:
            rand_y = self.np_random.uniform(-0.5, 0.5, size=(1, 1))[0][0]

        goal = np.array([[rand_x, rand_y]])
        return goal

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def sample_tasks(self):
        goals = self._sample_square_wth_nogo_zone()
        # goals = self.np_random.uniform(-0.5, 0.5, size=(1, 2))
        # goals = np.array(self.task_sequence)
        tasks = [{"goal": goal} for goal in goals]
        return tasks

    def reset_task(self, task):
        self._task = task
        self._goal = task["goal"]

    def reset(self):
        self._state = np.array(START)  # np.zeros(2, dtype=np.float32)
        self.horizon = HORIZON
        self.cummulative_reward = 0
        self.episode_x_path.clear()
        self.episode_y_path.clear()

        d2ng = dist_2_nogo(self._state[0], self._state[1])
        return (self._state, d2ng)

    def step(self, action):
        action = np.clip(action, -0.1, 0.1)
        assert self.action_space.contains(action)
        self._state = self._state + action

        delta_x = self._state[0] - self._goal[0]
        delta_y = self._state[1] - self._goal[1]
        reward = -np.sqrt(delta_x ** 2 + delta_y ** 2)

        # Check if the x and y are in the no-go zone
        # If yes, punish the agent.
        if is_nogo(self._state[0], self._state[1]):
            reward -= 100

        d2ng = dist_2_nogo(self._state[0], self._state[1])

        reached = (np.abs(delta_x) < 0.01) and (np.abs(delta_y) < 0.01)
        done = reached or self.horizon <= 0
        self.horizon -= 1
        self.cummulative_reward += reward

        self.episode_x_path.append(self._state[0])
        self.episode_y_path.append(self._state[1])

        return (
            (self._state, d2ng),
            reward,
            done,
            reached,
            self.cummulative_reward,
            self._task,
        )

    def render_episode(self):
        plt.figure()
        plt.plot(self.episode_x_path, self.episode_y_path)
        plt.plot(self._goal[0], self._goal[1], "r*")
        plt.show()
